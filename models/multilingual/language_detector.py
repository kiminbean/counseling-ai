"""
ë‹¤êµ­ì–´ ì–¸ì–´ ê°ì§€ ë° ë¬¸í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë“ˆ
Phase 3: êµ­ì œí™” í™•ì¥
ì €ì¥ ê²½ë¡œ: /AI_Drive/counseling_ai/models/multilingual/language_detector.py
"""
import tensorflow as tf
from tensorflow import keras
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
import re
from datetime import datetime

class SupportedLanguage(Enum):
    """ì§€ì› ì–¸ì–´ ëª©ë¡"""
    KOREAN = "ko"
    ENGLISH = "en"
    JAPANESE = "ja"
    CHINESE_SIMPLIFIED = "zh"
    CHINESE_TRADITIONAL = "zh-TW"
    VIETNAMESE = "vi"

class CommunicationStyle(Enum):
    """ë¬¸í™”ë³„ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤íƒ€ì¼"""
    HIGH_CONTEXT = "high_context" # í•œêµ­, ì¼ë³¸
    LOW_CONTEXT = "low_context"   # ì˜ì–´ê¶Œ
    INDIRECT = "indirect"         # ë™ì•„ì‹œì•„
    DIRECT = "direct"             # ì„œì–‘

@dataclass
class LanguageDetectionResult:
    """ì–¸ì–´ ê°ì§€ ê²°ê³¼"""
    primary_language: str
    confidence: float
    secondary_languages: List[Tuple[str, float]]
    script_type: str
    is_mixed: bool
    code_switching_detected: bool
    detected_phrases: Dict[str, List[str]]

@dataclass
class CulturalContext:
    """ë¬¸í™”ì  ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼"""
    language: str
    communication_style: CommunicationStyle
    formality_level: str # formal, informal, mixed
    emotional_expression_style: str # reserved, expressive
    family_orientation: str # collectivist, individualist
    stigma_sensitivity: float # 0-1
    recommended_approach: str
    cultural_considerations: List[str]
    taboo_topics: List[str]
    preferred_honorifics: Dict[str, str]

class MultilingualLanguageDetector(keras.Model):
    """
    ë‹¤êµ­ì–´ ì–¸ì–´ ê°ì§€ ë° ë¬¸í™” ë¶„ì„ ëª¨ë¸
    Features:
    - 6ê°œ ì–¸ì–´ ì§€ì› (ko, en, ja, zh, zh-TW, vi)
    - ì½”ë“œ ìŠ¤ìœ„ì¹­ ê°ì§€
    - ë¬¸í™”ì  ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
    - ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤íƒ€ì¼ íŒŒì•…
    """
    
    def __init__(self, vocab_size: int = 100000, embedding_dim: int = 256, num_languages: int = 6, max_length: int = 512, **kwargs):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.num_languages = num_languages
        self.max_length = max_length
        
        # ì–¸ì–´ë³„ íŠ¹ì„± ì •ì˜
        self.language_features = self._define_language_features()
        
        # ë¬¸í™”ë³„ ì»¨í…ìŠ¤íŠ¸ ì •ì˜
        self.cultural_contexts = self._define_cultural_contexts()
        
        # ëª¨ë¸ ë ˆì´ì–´ êµ¬ì¶•
        self._build_layers()

    def _define_language_features(self) -> Dict[str, Dict[str, Any]]:
        """ì–¸ì–´ë³„ íŠ¹ì„± ì •ì˜"""
        return {
            "ko": {
                "script_patterns": [r'[ê°€-í£]', r'[ã„±-ã…ã…-ã…£]'],
                "common_particles": ["ì´", "ê°€", "ì„", "ë¥¼", "ì€", "ëŠ”", "ì—", "ì—ì„œ", "ë¡œ", "ìœ¼ë¡œ"],
                "honorific_markers": ["ìš”", "ìŠµë‹ˆë‹¤", "ì„¸ìš”", "ì‹œ", "ë‹˜"],
                "emotion_markers": ["ã…œã…œ", "ã… ã… ", "ã…ã…", "ã„±ã„±", ";;"],
                "sentence_enders": ["ë‹¤", "ìš”", "ê¹Œ", "ë„¤", "êµ°"],
            },
            "en": {
                "script_patterns": [r'[a-zA-Z]'],
                "common_words": ["the", "is", "are", "was", "were", "have", "has", "been"],
                "emotion_markers": ["...", "!", "?!", ":(", ":)"],
                "contractions": ["'m", "'re", "'ve", "'ll", "'d", "n't"],
            },
            "ja": {
                "script_patterns": [r'[ã²ã‚‰ãŒãª]', r'[ã‚«ã‚¿ã‚«ãƒŠ]', r'[ä¸€-é¾¯]'],
                "hiragana": r'[ã€-ã‚Ÿ]',
                "katakana": r'[ã‚ -ãƒ¿]',
                "common_particles": ["ã¯", "ãŒ", "ã‚’", "ã«", "ã§", "ã¨", "ã‚‚", "ã®"],
                "honorific_markers": ["ã§ã™", "ã¾ã™", "ã”ã–ã„ã¾ã™", "ã•ã‚“", "æ§˜"],
                "emotion_markers": ["ï¼ˆç¬‘ï¼‰", "w", "orz", "^^"],
            },
            "zh": {
                "script_patterns": [r'[ä¸€-é¿¿]'],
                "simplified_specific": ["ä»¬", "è¿™", "é‚£", "å›½", "å­¦"],
                "common_particles": ["çš„", "äº†", "æ˜¯", "åœ¨", "æœ‰", "å’Œ"],
                "emotion_markers": ["å“ˆå“ˆ", "å‘µå‘µ", "å˜¿å˜¿", "ã€‚ã€‚ã€‚"],
            },
            "zh-TW": {
                "script_patterns": [r'[ä¸€-é¿¿]'],
                "traditional_specific": ["å€‘", "é€™", "åœ‹", "å­¸", "ç‚º"],
                "common_particles": ["çš„", "äº†", "æ˜¯", "åœ¨", "æœ‰", "å’Œ"],
            },
            "vi": {
                "script_patterns": [r'[Ã Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘]'],
                "tone_markers": ["Ã ", "Ã¡", "áº£", "Ã£", "áº¡", "Äƒ", "Ã¢"],
                "common_words": ["lÃ ", "cá»§a", "vÃ ", "cÃ³", "Ä‘Æ°á»£c", "trong", "nÃ y"],
            }
        }

    def _define_cultural_contexts(self) -> Dict[str, CulturalContext]:
        """ë¬¸í™”ë³„ ì»¨í…ìŠ¤íŠ¸ ì •ì˜"""
        return {
            "ko": CulturalContext(
                language="ko",
                communication_style=CommunicationStyle.HIGH_CONTEXT,
                formality_level="formal",
                emotional_expression_style="reserved",
                family_orientation="collectivist",
                stigma_sensitivity=0.8,
                recommended_approach="indirect_supportive",
                cultural_considerations=[
                    "ì²´ë©´(face) ì¤‘ì‹œ",
                    "ì •(jeong) - ì •ì„œì  ìœ ëŒ€ê°",
                    "ëˆˆì¹˜(nunchi) - ë¹„ì–¸ì–´ì  ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",
                    "í•œ(han) - ê¹Šì€ ìŠ¬í””ê³¼ ì–µìš¸í•¨",
                    "íš¨(hyo) - ë¶€ëª¨ ê³µê²½",
                    "ê°€ì¡± ì¤‘ì‹¬ ë¬¸í™”",
                    "ì„¸ëŒ€ ê°„ ê°ˆë“± ì´í•´"
                ],
                taboo_topics=[
                    "ì§ì ‘ì ì¸ ì •ì‹ ê±´ê°• ì–¸ê¸‰ (ì´ˆê¸°)",
                    "ê°€ì¡± ë¹„íŒ",
                    "ì‚¬íšŒì  ì‹¤íŒ¨"
                ],
                preferred_honorifics={
                    "greeting": "ì•ˆë…•í•˜ì„¸ìš”",
                    "acknowledgment": "ë„¤, ì´í•´í•©ë‹ˆë‹¤",
                    "empathy": "ë§ì´ í˜ë“œì…¨ê² ì–´ìš”",
                    "closing": "í¸ì•ˆí•œ í•˜ë£¨ ë³´ë‚´ì„¸ìš”"
                }
            ),
            "en": CulturalContext(
                language="en",
                communication_style=CommunicationStyle.LOW_CONTEXT,
                formality_level="informal",
                emotional_expression_style="expressive",
                family_orientation="individualist",
                stigma_sensitivity=0.4,
                recommended_approach="direct_supportive",
                cultural_considerations=[
                    "Individual autonomy valued",
                    "Direct communication preferred",
                    "Emotional expression encouraged",
                    "Self-help orientation",
                    "Work-life balance awareness"
                ],
                taboo_topics=[],
                preferred_honorifics={
                    "greeting": "Hello",
                    "acknowledgment": "I understand",
                    "empathy": "That sounds really difficult",
                    "closing": "Take care"
                }
            ),
            "ja": CulturalContext(
                language="ja",
                communication_style=CommunicationStyle.HIGH_CONTEXT,
                formality_level="formal",
                emotional_expression_style="reserved",
                family_orientation="collectivist",
                stigma_sensitivity=0.85,
                recommended_approach="indirect_respectful",
                cultural_considerations=[
                    "å’Œ(wa) - ì¡°í™” ì¤‘ì‹œ",
                    "æœ¬éŸ³ã¨å»ºå‰(honne/tatemae) - ì†ë§ˆìŒê³¼ ê²‰ëª¨ìŠµ",
                    "æˆ‘æ…¢(gaman) - ì¸ë‚´",
                    "ç©ºæ°—ã‚’èª­ã‚€(kuuki wo yomu) - ë¶„ìœ„ê¸° íŒŒì•…",
                    "æ¥(haji) - ìˆ˜ì¹˜ì‹¬ ë¬¸í™”"
                ],
                taboo_topics=[
                    "ì§ì ‘ì ì¸ ê°ì • í‘œí˜„ ê°•ìš”",
                    "ê°€ì¡± ë¬¸ì œ ì§ì ‘ ì–¸ê¸‰"
                ],
                preferred_honorifics={
                    "greeting": "ã“ã‚“ã«ã¡ã¯",
                    "acknowledgment": "ã¯ã„ã€ã‚ã‹ã‚Šã¾ã™",
                    "empathy": "ãã‚Œã¯å¤§å¤‰ã§ã—ãŸã­",
                    "closing": "ãŠä½“ã‚’ãŠå¤§äº‹ã«"
                }
            ),
            "zh": CulturalContext(
                language="zh",
                communication_style=CommunicationStyle.HIGH_CONTEXT,
                formality_level="mixed",
                emotional_expression_style="reserved",
                family_orientation="collectivist",
                stigma_sensitivity=0.75,
                recommended_approach="indirect_warm",
                cultural_considerations=[
                    "é¢å­(miÃ nzi) - ì²´ë©´",
                    "å…³ç³»(guÄnxi) - ê´€ê³„ ì¤‘ì‹œ",
                    "å­é¡º(xiÃ oshÃ¹n) - íš¨ë„",
                    "å®¶åº­ ì¤‘ì‹¬ ê°€ì¹˜ê´€"
                ],
                taboo_topics=[
                    "ê°€ì¡± ë¹„íŒ",
                    "ì§ì ‘ì  ì •ì‹ ê±´ê°• ì§„ë‹¨"
                ],
                preferred_honorifics={
                    "greeting": "æ‚¨å¥½",
                    "acknowledgment": "æˆ‘ç†è§£",
                    "empathy": "è¿™ä¸€å®šå¾ˆä¸å®¹æ˜“",
                    "closing": "ç¥æ‚¨ä¸€åˆ‡é¡ºåˆ©"
                }
            )
        }

    def _build_layers(self):
        """ëª¨ë¸ ë ˆì´ì–´ êµ¬ì¶•"""
        # ì„ë² ë”© ë ˆì´ì–´
        self.token_embedding = keras.layers.Embedding(
            self.vocab_size, self.embedding_dim, name="token_embedding"
        )
        
        # ë¬¸ì ìˆ˜ì¤€ CNN
        self.char_cnn = keras.Sequential([
            keras.layers.Conv1D(128, 3, activation='relu', padding='same'),
            keras.layers.Conv1D(128, 5, activation='relu', padding='same'),
            keras.layers.GlobalMaxPooling1D()
        ], name="char_cnn")
        
        # BiLSTM ë ˆì´ì–´
        self.bilstm = keras.layers.Bidirectional(
            keras.layers.LSTM(128, return_sequences=True),
            name="bilstm"
        )
        
        # ì–¸ì–´ ë¶„ë¥˜ í—¤ë“œ
        self.language_classifier = keras.Sequential([
            keras.layers.GlobalAveragePooling1D(),
            keras.layers.Dense(256, activation='relu'),
            keras.layers.Dropout(0.3),
            keras.layers.Dense(128, activation='relu'),
            keras.layers.Dense(self.num_languages, activation='softmax')
        ], name="language_classifier")
        
        # ì½”ë“œ ìŠ¤ìœ„ì¹­ ê°ì§€ í—¤ë“œ
        self.code_switch_detector = keras.Sequential([
            keras.layers.Dense(64, activation='relu'),
            keras.layers.Dense(1, activation='sigmoid')
        ], name="code_switch_detector")
        
        # í˜•ì‹ì„± ë¶„ë¥˜ í—¤ë“œ
        self.formality_classifier = keras.Sequential([
            keras.layers.GlobalAveragePooling1D(),
            keras.layers.Dense(64, activation='relu'),
            keras.layers.Dense(3, activation='softmax') # formal, informal, mixed
        ], name="formality_classifier")

    def call(self, inputs: Dict[str, tf.Tensor], training: bool = False) -> Dict[str, tf.Tensor]:
        """
        ìˆœì „íŒŒ
        Args:
            inputs: {
                'input_ids': (batch, seq_len),
                'char_ids': (batch, seq_len, char_len)
            }
            training: í•™ìŠµ ëª¨ë“œ ì—¬ë¶€
        Returns:
            ì–¸ì–´ í™•ë¥ , ì½”ë“œ ìŠ¤ìœ„ì¹­ í™•ë¥ , í˜•ì‹ì„± í™•ë¥ 
        """
        input_ids = inputs['input_ids']
        
        # í† í° ì„ë² ë”©
        token_emb = self.token_embedding(input_ids)
        
        # BiLSTM ì²˜ë¦¬
        lstm_out = self.bilstm(token_emb, training=training)
        
        # ì–¸ì–´ ë¶„ë¥˜
        language_probs = self.language_classifier(lstm_out)
        
        # ì½”ë“œ ìŠ¤ìœ„ì¹­ ê°ì§€
        pooled = tf.reduce_mean(lstm_out, axis=1)
        code_switch_prob = self.code_switch_detector(pooled)
        
        # í˜•ì‹ì„± ë¶„ë¥˜
        formality_probs = self.formality_classifier(lstm_out)
        
        return {
            'language_probs': language_probs,
            'code_switch_prob': code_switch_prob,
            'formality_probs': formality_probs,
            'hidden_states': lstm_out
        }

    def detect_language(self, text: str, tokenizer: Any = None) -> LanguageDetectionResult:
        """
        í…ìŠ¤íŠ¸ ì–¸ì–´ ê°ì§€
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            tokenizer: í† í¬ë‚˜ì´ì € (Noneì´ë©´ ê·œì¹™ ê¸°ë°˜ ì‚¬ìš©)
        Returns:
            LanguageDetectionResult
        """
        # ê·œì¹™ ê¸°ë°˜ ë¹ ë¥¸ ê°ì§€
        rule_based = self._rule_based_detection(text)
        
        # ëª¨ë¸ ê¸°ë°˜ ê°ì§€ (tokenizerê°€ ìˆëŠ” ê²½ìš°)
        if tokenizer is not None:
            model_based = self._model_based_detection(text, tokenizer)
            # ì•™ìƒë¸”
            final_result = self._ensemble_results(rule_based, model_based)
        else:
            final_result = rule_based
            
        return final_result

    def _rule_based_detection(self, text: str) -> LanguageDetectionResult:
        """ê·œì¹™ ê¸°ë°˜ ì–¸ì–´ ê°ì§€"""
        scores = {lang: 0.0 for lang in SupportedLanguage}
        detected_phrases = {lang.value: [] for lang in SupportedLanguage}
        
        # í•œêµ­ì–´ ê°ì§€
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        if korean_chars > 0:
            scores[SupportedLanguage.KOREAN] = korean_chars / len(text) if text else 0
            
        # ì¼ë³¸ì–´ ê°ì§€ (íˆë¼ê°€ë‚˜/ì¹´íƒ€ì¹´ë‚˜)
        hiragana = len(re.findall(r'[\u3040-\u309F]', text))
        katakana = len(re.findall(r'[\u30A0-\u30FF]', text))
        if hiragana + katakana > 0:
            scores[SupportedLanguage.JAPANESE] = (hiragana + katakana) / len(text) if text else 0
            
        # ì¤‘êµ­ì–´ ê°ì§€ (í•œì)
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        # í•œêµ­ì–´/ì¼ë³¸ì–´ê°€ ì•„ë‹Œ ê²½ìš°ì˜ í•œì
        if chinese_chars > 0 and scores[SupportedLanguage.KOREAN] < 0.1 and scores[SupportedLanguage.JAPANESE] < 0.1:
            # ê°„ì²´/ë²ˆì²´ êµ¬ë¶„
            simplified = len(re.findall(r'[ä»¬è¿™é‚£å›½å­¦ä¸ºä¼š]', text))
            traditional = len(re.findall(r'[å€‘é€™åœ‹å­¸ç‚ºæœƒ]', text))
            if simplified > traditional:
                scores[SupportedLanguage.CHINESE_SIMPLIFIED] = chinese_chars / len(text) if text else 0
            else:
                scores[SupportedLanguage.CHINESE_TRADITIONAL] = chinese_chars / len(text) if text else 0
                
        # ë² íŠ¸ë‚¨ì–´ ê°ì§€
        vietnamese_chars = len(re.findall(r'[Ã Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘Ä]', text))
        if vietnamese_chars > 0:
            scores[SupportedLanguage.VIETNAMESE] = vietnamese_chars / len(text) if text else 0
            
        # ì˜ì–´ ê°ì§€
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        english_words = len(re.findall(r'\b(the|is|are|was|were|have|has|been|I|you|we|they|it|this|that)\b', text.lower()))
        if english_chars > 0 and sum([scores[lang] for lang in scores if lang != SupportedLanguage.ENGLISH]) < 0.3:
            scores[SupportedLanguage.ENGLISH] = (english_chars / len(text) * 0.5 + min(english_words / 10, 0.5)) if text else 0
            
        # ê²°ê³¼ ì •ë ¬
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        primary = sorted_scores[0]
        
        # ì½”ë“œ ìŠ¤ìœ„ì¹­ ê°ì§€
        significant_languages = [lang for lang, score in sorted_scores if score > 0.1]
        is_mixed = len(significant_languages) > 1
        
        return LanguageDetectionResult(
            primary_language=primary[0].value,
            confidence=primary[1],
            secondary_languages=[(lang.value, score) for lang, score in sorted_scores[1:] if score > 0.05],
            script_type=self._detect_script_type(text),
            is_mixed=is_mixed,
            code_switching_detected=is_mixed,
            detected_phrases=detected_phrases
        )

    def _model_based_detection(self, text: str, tokenizer: Any) -> LanguageDetectionResult:
        """ëª¨ë¸ ê¸°ë°˜ ì–¸ì–´ ê°ì§€"""
        # í† í°í™”
        encoded = tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='tf'
        )
        
        # ëª¨ë¸ ì¶”ë¡ 
        outputs = self({
            'input_ids': encoded['input_ids']
        }, training=False)
        
        # ì–¸ì–´ í™•ë¥  ì¶”ì¶œ
        lang_probs = outputs['language_probs'][0].numpy()
        code_switch = outputs['code_switch_prob'][0].numpy()[0]
        
        # ì–¸ì–´ ë§¤í•‘
        lang_map = {
            0: "ko", 1: "en", 2: "ja", 3: "zh", 4: "zh-TW", 5: "vi"
        }
        
        sorted_indices = np.argsort(lang_probs)[::-1]
        primary_lang = lang_map[sorted_indices[0]]
        primary_conf = float(lang_probs[sorted_indices[0]])
        
        secondary = [
            (lang_map[idx], float(lang_probs[idx]))
            for idx in sorted_indices[1:]
            if lang_probs[idx] > 0.05
        ]
        
        return LanguageDetectionResult(
            primary_language=primary_lang,
            confidence=primary_conf,
            secondary_languages=secondary,
            script_type=self._detect_script_type(text),
            is_mixed=code_switch > 0.5,
            code_switching_detected=code_switch > 0.5,
            detected_phrases={}
        )

    def _ensemble_results(
        self,
        rule_based: LanguageDetectionResult,
        model_based: LanguageDetectionResult
    ) -> LanguageDetectionResult:
        """ê·œì¹™ ê¸°ë°˜ê³¼ ëª¨ë¸ ê¸°ë°˜ ê²°ê³¼ ì•™ìƒë¸”"""
        # ê°€ì¤‘ í‰ê· 
        rule_weight = 0.4
        model_weight = 0.6
        
        if rule_based.primary_language == model_based.primary_language:
            # ì¼ì¹˜í•˜ëŠ” ê²½ìš°
            confidence = (
                rule_based.confidence * rule_weight +
                model_based.confidence * model_weight
            )
            return LanguageDetectionResult(
                primary_language=rule_based.primary_language,
                confidence=confidence,
                secondary_languages=model_based.secondary_languages,
                script_type=rule_based.script_type,
                is_mixed=rule_based.is_mixed or model_based.is_mixed,
                code_switching_detected=model_based.code_switching_detected,
                detected_phrases=rule_based.detected_phrases
            )
        else:
            # ë¶ˆì¼ì¹˜í•˜ëŠ” ê²½ìš° - ë” ë†’ì€ ì‹ ë¢°ë„ ì„ íƒ
            if rule_based.confidence > model_based.confidence:
                return rule_based
            else:
                return model_based

    def _detect_script_type(self, text: str) -> str:
        """ë¬¸ì ì²´ê³„ ê°ì§€"""
        if re.search(r'[ê°€-í£]', text):
            return "hangul"
        elif re.search(r'[\u3040-\u309F]', text):
            return "hiragana"
        elif re.search(r'[\u30A0-\u30FF]', text):
            return "katakana"
        elif re.search(r'[\u4e00-\u9fff]', text):
            return "hanzi"
        elif re.search(r'[Ã Ã¡áº£Ã£áº¡Äƒáº±áº¯áº³áºµáº·Ã¢áº§áº¥áº©áº«áº­]', text):
            return "vietnamese_latin"
        else:
            return "latin"

    def analyze_cultural_context(self, text: str, detected_language: str) -> CulturalContext:
        """
        ë¬¸í™”ì  ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            detected_language: ê°ì§€ëœ ì–¸ì–´ ì½”ë“œ
        Returns:
            CulturalContext
        """
        # ê¸°ë³¸ ë¬¸í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        base_context = self.cultural_contexts.get(
            detected_language,
            self.cultural_contexts["en"] # ê¸°ë³¸ê°’
        )
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì¶”ê°€ ë¬¸í™”ì  ë‹¨ì„œ ë¶„ì„
        formality = self._analyze_formality(text, detected_language)
        emotional_style = self._analyze_emotional_style(text, detected_language)
        
        # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        return CulturalContext(
            language=detected_language,
            communication_style=base_context.communication_style,
            formality_level=formality,
            emotional_expression_style=emotional_style,
            family_orientation=base_context.family_orientation,
            stigma_sensitivity=base_context.stigma_sensitivity,
            recommended_approach=base_context.recommended_approach,
            cultural_considerations=base_context.cultural_considerations,
            taboo_topics=base_context.taboo_topics,
            preferred_honorifics=base_context.preferred_honorifics
        )

    def _analyze_formality(self, text: str, language: str) -> str:
        """í˜•ì‹ì„± ë¶„ì„"""
        if language == "ko":
            # í•œêµ­ì–´ í˜•ì‹ì„±
            formal_markers = ["ìŠµë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ì„¸ìš”", "ì‹œ"]
            informal_markers = ["ì–´", "ì•¼", "ëƒ", "ã…‹", "ã…"]
            
            formal_count = sum(1 for m in formal_markers if m in text)
            informal_count = sum(1 for m in informal_markers if m in text)
            
            if formal_count > informal_count:
                return "formal"
            elif informal_count > formal_count:
                return "informal"
            else:
                return "mixed"
                
        elif language == "ja":
            # ì¼ë³¸ì–´ í˜•ì‹ì„±
            formal_markers = ["ã§ã™", "ã¾ã™", "ã”ã–ã„ã¾ã™"]
            informal_markers = ["ã ", "ã‚ˆ", "ã­"]
            
            formal_count = sum(1 for m in formal_markers if m in text)
            informal_count = sum(1 for m in informal_markers if m in text)
            
            if formal_count > informal_count:
                return "formal"
            elif informal_count > formal_count:
                return "informal"
            else:
                return "mixed"
        else:
            return "neutral"

    def _analyze_emotional_style(self, text: str, language: str) -> str:
        """ê°ì • í‘œí˜„ ìŠ¤íƒ€ì¼ ë¶„ì„"""
        # ê°ì • í‘œí˜„ ë§ˆì»¤
        expressive_markers = ["!", "!!", "...", "ã…œã…œ", "ã… ã… ", "ğŸ˜¢", "ğŸ˜­"]
        reserved_indicators = len(text) > 50 and text.count("!") < 2
        
        expressive_count = sum(1 for m in expressive_markers if m in text)
        
        if expressive_count > 3:
            return "expressive"
        elif reserved_indicators:
            return "reserved"
        else:
            return "moderate"

    def get_therapeutic_recommendations(
        self,
        cultural_context: CulturalContext
    ) -> Dict[str, Any]:
        """
        ë¬¸í™”ì  ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¹˜ë£Œì  ê¶Œê³ ì‚¬í•­
        Args:
            cultural_context: ë¬¸í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼
        Returns:
            ì¹˜ë£Œì  ê¶Œê³ ì‚¬í•­
        """
        recommendations = {
            "communication_approach": "",
            "initial_rapport": [],
            "therapeutic_techniques": [],
            "things_to_avoid": [],
            "crisis_response_adaptation": {}
        }
        
        if cultural_context.communication_style == CommunicationStyle.HIGH_CONTEXT:
            recommendations["communication_approach"] = "ê°„ì ‘ì ì´ê³  ë§¥ë½ì„ ê³ ë ¤í•œ ì ‘ê·¼"
            recommendations["initial_rapport"] = [
                "ì¶©ë¶„í•œ ì‹œê°„ì„ ë‘ê³  ì‹ ë¢° êµ¬ì¶•",
                "ë¹„ì–¸ì–´ì  ì‹ í˜¸ì— ì£¼ì˜",
                "ì„±ê¸‰í•œ ë¬¸ì œ í•´ê²° ì§€ì–‘"
            ]
            recommendations["therapeutic_techniques"] = [
                "ì€ìœ ì™€ ì´ì•¼ê¸° í™œìš©",
                "ê°€ì¡± ì²´ê³„ ê³ ë ¤",
                "ì ì§„ì  ìê¸° ê°œë°© ìœ ë„"
            ]
        else:
            recommendations["communication_approach"] = "ì§ì ‘ì ì´ê³  ëª…í™•í•œ ì ‘ê·¼"
            recommendations["initial_rapport"] = [
                "ëª©í‘œ ì§€í–¥ì  ëŒ€í™”",
                "ê°ì • í‘œí˜„ ê²©ë ¤",
                "ìê¸° íš¨ëŠ¥ê° ê°•ì¡°"
            ]
            recommendations["therapeutic_techniques"] = [
                "êµ¬ì²´ì  í–‰ë™ ê³„íš",
                "ì¸ì§€ ì¬êµ¬ì¡°í™”",
                "ìê¸° ì£¼ì¥ í›ˆë ¨"
            ]
            
        if cultural_context.stigma_sensitivity > 0.7:
            recommendations["things_to_avoid"] = [
                "ì´ˆê¸°ì— 'ì •ì‹ ê±´ê°•' ì§ì ‘ ì–¸ê¸‰",
                "ì§„ë‹¨ëª… ì‚¬ìš©",
                "ê°€ì¡±ì—ê²Œ ì•Œë¦´ ê²ƒì„ ê°•ìš”"
            ]
            recommendations["crisis_response_adaptation"] = {
                "approach": "ë¶€ë“œëŸ½ê³  ë¹„ë‚™ì¸ì ",
                "language": "ì¼ìƒì  ì–´íœ˜ ì‚¬ìš©",
                "framing": "ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ ê´€ì "
            }
            
        return recommendations

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # ëª¨ë¸ ì´ˆê¸°í™”
    detector = MultilingualLanguageDetector()
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_texts = [
        "ì˜¤ëŠ˜ í•˜ë£¨ê°€ ë„ˆë¬´ í˜ë“¤ì—ˆì–´ìš”. ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš”.",
        "I'm feeling overwhelmed with work and life.",
        "ì˜¤ëŠ˜ì€ ì •ë§ í”¼ê³¤í–ˆì–´ìš”. ì•„ë¬´ê²ƒë„ í•˜ê³  ì‹¶ì§€ ì•Šì•„ìš”.",
        "æˆ‘ä»Šå¤©æ„Ÿè§‰å¾ˆç´¯ï¼Œä»€ä¹ˆéƒ½ä¸æƒ³åšã€‚",
        "TÃ´i cáº£m tháº¥y ráº¥t má»‡t má»i hÃ´m nay.",
        "ìš”ì¦˜ I feel so tired ë§¤ì¼ í˜ë“¤ì–´ìš”" # ì½”ë“œ ìŠ¤ìœ„ì¹­
    ]
    
    print("=== ë‹¤êµ­ì–´ ì–¸ì–´ ê°ì§€ í…ŒìŠ¤íŠ¸ ===\n")
    for text in test_texts:
        result = detector.detect_language(text)
        print(f"í…ìŠ¤íŠ¸: {text}")
        print(f" ì–¸ì–´: {result.primary_language} (ì‹ ë¢°ë„: {result.confidence:.2f})")
        print(f" ì½”ë“œ ìŠ¤ìœ„ì¹­: {result.code_switching_detected}")
        
        # ë¬¸í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        context = detector.analyze_cultural_context(text, result.primary_language)
        print(f" ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤íƒ€ì¼: {context.communication_style.value}")
        print(f" ë‚™ì¸ ë¯¼ê°ë„: {context.stigma_sensitivity}")
        print()
